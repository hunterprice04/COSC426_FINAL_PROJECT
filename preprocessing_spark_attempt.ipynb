{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### References\n",
    "[1] https://nirvan66.github.io/geoguessr.html\n",
    "We took inspiration from this previous project and attempted to improve the results and collect more data. Our main contributions to the training portion of the project were:\n",
    "- include creating a custom training loop and a custom loss function.\n",
    "- Parallelizing the model itself with tensorflow distributed.\n",
    "- We attempted to train the entirety of the model all in spark such that it is entirely distributed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attempt at using Spark for entire Pipeline\n",
    "This file contains our attempt at using Spark for the entire preprocessing and modeling pipeline.\n",
    "We were successful in implementing the preprocessing portion but had consistent issues with running out of memory.\n",
    "Additionally, we were unable to get any libraries to work that we needed to use keras with spark.\n",
    "We implement a working version of this pipeline in pure numpy and tensorflow in main.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "04991374",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "04991374"
   },
   "source": [
    "## Import All Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from numpy import asarray\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from PIL import Image\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import rand\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D, Reshape\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Dropout, LSTM\n",
    "from pyspark.ml import Pipeline\n",
    "from distkeras import predictors, trainers\n",
    "from pyspark.sql import functions, types\n",
    "from pyspark import ml\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download image data\n",
    "The following cell downloads the Data tar file located in the team's Google Drive. This allows one to download all of the data necessary used for training and validating the model. After running the cell below, one should check the directory that they are in to verify that a directory and tar file name \"Data\" and \"Data.tar\" respectively appear. These files contain the images collected from the data scraping performed.\n",
    "\n",
    "**(NOTE: If you receive an error that prevents you from accessing the Google Drive, restart the kernel and rerun the cell once. If this does to fix the issue, please email Hunter Price for access information)**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ccaccc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81ccaccc",
    "outputId": "979567ce-312f-48ab-f908-16085e8ebc86"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_tar = 'Data.tar'\n",
    "data_url = 'https://drive.google.com/uc?id=13JA-0Pafd5VbCXEdnxJZw8XbMqcurxSg'\n",
    "\n",
    "if not os.path.exists(data_tar) and not os.path.exists('Data'):\n",
    "    gdown.download(data_url, data_tar, quiet=False)\n",
    "if not os.path.exists('Data'):\n",
    "    !tar -xf Data.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7fc17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "3fc7fc17"
   },
   "source": [
    "# Create CSV\n",
    "The following could handles parsing through data directory and creating a CSV of the filenames and metadata. One must first specify the directory that contain the images. This variable is preset to 'Data', which accesses the directory downloaded in the cell above. Once a directory is specified, one can run the cell to download the dataset into a CSV file named \"DataLabels.csv\". Essentially, the code uses the directory names within the data directory to label the data accordingly. The code also filters out any image folders that do not contain all three images. Please verify that the CSV file has been properly created before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d028560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d028560",
    "outputId": "6f2100fe-94de-4147-9cdd-06898ae1cbac"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created a CSV file named DataLabels.csv\n",
      " \n",
      "The following paths will not be included because they contain less than 3 directions:\n",
      "Data/39_-116.0_44.0_-114.0_46.0/45.5970459,-115.861618\n",
      "Data/0_-124.552441_39.97720877884329_-124.0_45.08734407897483/40.07824520959485,-124.0863631072246\n"
     ]
    }
   ],
   "source": [
    "#Declared global variables\n",
    "dir_name = 'Data'\n",
    "myDict = {}\n",
    "data_frame = pd.DataFrame()\n",
    "\n",
    "#This function gets the pathnames to each of the images\n",
    "def get_filepaths(directory):\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "    count = 0\n",
    "    \n",
    "    # Walk the tree.\n",
    "    print(\" \")\n",
    "    print(\"The following paths will not be included because they contain less than 3 directions:\")\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        if(len(files) == 3 or len(files) == 0):\n",
    "            for filename in files:\n",
    "                count += 1\n",
    "                # Join the two strings in order to form the full filepath.\n",
    "                filepath = os.path.join(root, filename)\n",
    "                count, file_paths.append(filepath) \n",
    "        else:\n",
    "            print(str(root))\n",
    "    return count, file_paths\n",
    "\n",
    "#This function parses the path names and extracts the information\n",
    "def data_to_labels(dir_name):\n",
    "    counted, file_paths = (get_filepaths(dir_name))\n",
    "    file_paths_segmented = [f.split(os.path.sep) for f in file_paths]\n",
    "    file_segmented2 = []\n",
    "\n",
    "    for i, f in enumerate(file_paths_segmented):\n",
    "        files = []\n",
    "        for fi in f:\n",
    "            files.append(fi.replace('_',',').split(','))\n",
    "\n",
    "        files = reduce(lambda a,b:a+b, files)\n",
    "        files.insert(0,file_paths[i])\n",
    "        file_segmented2.append(files)    \n",
    "        myDict[\"File \" + str(i)] = files\n",
    "    return myDict\n",
    "\n",
    "#Main simply stores the information in a CSV. This process will be turned into a seperate function\n",
    "def main():\n",
    "    \n",
    "    #Fill the dir_name string manually to run in notebook\n",
    "    if(dir_name == ''):\n",
    "        if(len(sys.argv) != 2):\n",
    "            print('Please specify a directory or Define \"dir_path\" in the code')   \n",
    "        else:\n",
    "            print(f\"Created a CSV file named DataLabels.csv\")\n",
    "            data_frame = (pd.DataFrame.from_dict(data_to_labels(str(sys.argv[1])),orient='columns')).T\n",
    "            data_frame.columns = ['File Path', 'Data Folder', 'Grid Number', 'Min X', 'Min Y', 'Max X', 'Max Y','Latitude','Longitude','Angle','File Name'] \n",
    "            data_frame.to_csv('DataLabels.csv')\n",
    "    else:\n",
    "            print(f\"Created a CSV file named DataLabels.csv\")\n",
    "            data_frame = (pd.DataFrame.from_dict(data_to_labels(dir_name),orient='columns')).T\n",
    "            data_frame.columns = ['File Path', 'Data Folder', 'Grid Number', 'Min X', 'Min Y', 'Max X', 'Max Y','Latitude','Longitude','Angle','File Name'] \n",
    "            data_frame.to_csv('DataLabels.csv')\n",
    "    \n",
    "if __name__==\"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Spark Context and Session\n",
    "This creates the local spark context then creates a sql session with the spark context. This is needed as it gives us more functions otherwise unavailable with only the spark context."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51789ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "f51789ce"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a spark context and a spark session with some extra config stuff\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.getConf().set('spark.executor.memory', '6g') \\\n",
    "    .set(\"spark.driver.memory\", \"16g\") \\\n",
    "    .set(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .set(\"spark.memory.offHeap.size\",'6g')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define map functions for spark\n",
    "The following functions are used to define the map functions for the spark dataframes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f03177",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a7f03177"
   },
   "outputs": [],
   "source": [
    "# This takes in a row of pandas series and converts it to a single dict of the values\n",
    "def get_row(r):\n",
    "    row = {\n",
    "            'File Path 1': r['File Path'].iloc[0],\n",
    "            'File Path 2': r['File Path'].iloc[1],\n",
    "            'File Path 3': r['File Path'].iloc[2],\n",
    "            'Latitude': r['Latitude'].iloc[0],\n",
    "            'Longitude': r['Longitude'].iloc[0],\n",
    "            'Min Y': r['Min Y'].iloc[0],\n",
    "            'Min X': r['Min X'].iloc[0],\n",
    "            'Max Y': r['Max Y'].iloc[0],\n",
    "            'Max X': r['Max X'].iloc[0],\n",
    "            'Grid Number': int(r['Grid Number'].iloc[0])\n",
    "        }\n",
    "    return row\n",
    "\n",
    "# This loads in the image files and returns the data needed for the model\n",
    "from pyspark.sql.functions import array\n",
    "def load_data(r):\n",
    "    dsize = (200,100)\n",
    "    # Initialize the data vectors\n",
    "    X = np.zeros((3,dsize[1],dsize[0],3))\n",
    "    X[0,:] = asarray(Image.open(r['File Path 1']).resize(dsize, resample=Image.LANCZOS))\n",
    "    X[1,:] = asarray(Image.open(r['File Path 2']).resize(dsize, resample=Image.LANCZOS))\n",
    "    X[2,:] = asarray(Image.open(r['File Path 3']).resize(dsize, resample=Image.LANCZOS))\n",
    "\n",
    "    # Scale it and Flatten it here because it's a pain to do later\n",
    "    X /= 255\n",
    "    X = X.flatten()\n",
    "\n",
    "    # Grab remaining data\n",
    "    y = int(r['Grid Number'])\n",
    "    loc = np.array([r['Latitude'], r['Longitude']])\n",
    "\n",
    "    return X.tolist(), y, loc.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66678791",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "66678791"
   },
   "source": [
    "# Load in all data\n",
    "Read in all data specified in the csv into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "737ffef4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "737ffef4",
    "outputId": "56d95f69-8205-4192-9634-9628286d52d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- X: vector (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      " |-- loc: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data, then create a list containing a group of 3 files for all the files in data\n",
    "clean_data = pd.DataFrame(columns=['File Path 1', 'File Path 2', 'File Path 3',\n",
    "                                   'Latitude', 'Longitude',\n",
    "                                   'Min Y', 'Min X', 'Max Y', 'Max X', 'Grid Number'])\n",
    "data = pd.read_csv('DataLabels.csv')\n",
    "data = [data[i:i+3] for i in range(0, len(data), 3)]\n",
    "\n",
    "# create an rdd to read in the data\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.map(get_row)\n",
    "rdd = rdd.map(load_data)\n",
    "\n",
    "# use the rdd to create a structured dataframe\n",
    "columns = ['X', 'y', 'loc']\n",
    "df = spark.createDataFrame(rdd, columns)\n",
    "\n",
    "# Convert the array column types to vectors\n",
    "df = df.withColumn('X', array_to_vector('X'))\n",
    "df = df.withColumn('loc', array_to_vector('loc'))\n",
    "\n",
    "# TODO - throw this parquet into the Google Drive for download to speed up the process\n",
    "# if we want to save the partially preprocessed data do it here\n",
    "checkpoint_data = False\n",
    "if checkpoint_data:\n",
    "    df.write.parquet('loaded_data.parquet')\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Checkpoint\n",
    "We make a checkpoint of our data to make sure it's all good as well as to make sure we don't have to rerun the preprocessing as it consistently ran out of memory or took too long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0598f514",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0598f514",
    "outputId": "29ba6c2d-54a2-445b-deee-2a2653a297d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- X: vector (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      " |-- loc: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# download the partially preprocessed data\n",
    "download_data = False\n",
    "checkpoint_data = False\n",
    "\n",
    "if download_data:\n",
    "    data_tar = 'loaded_data.tar'\n",
    "    data_url = 'https://drive.google.com/uc?id=17YmXWu_K1b9o-VBjDFXLdX8WMfrRXA6b'\n",
    "\n",
    "    if not os.path.exists(data_tar) and not os.path.exists('loaded_data.parquet'):\n",
    "        gdown.download(data_url, data_tar, quiet=False)\n",
    "    if not os.path.exists('loaded_data.parquet'):\n",
    "        !tar -xf 'loaded_data.tar'\n",
    "    checkpoint_data = True\n",
    "\n",
    "# load in the partially preprocessed data\n",
    "if checkpoint_data:\n",
    "    df = spark.read.parquet(\"loaded_data.parquet\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5bf83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "e5b5bf83"
   },
   "source": [
    "# Build Preprocessing Pipeline\n",
    "This creates a pipeline to preprocess any data that has not yet been preprocessed. This one hot encodes the y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adff32a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "adff32a4",
    "outputId": "67338ad4-5648-47f7-b6f6-3e099138570d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-0f4864497058>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m ]\n\u001B[1;32m      9\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstages\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mpipeline_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0mdf_transform\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    112\u001B[0m                     \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# must be an Estimator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m                     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m                     \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mindexOfLastEstimator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \"\"\"\n\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1318\u001B[0m             \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1320\u001B[0;31m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m         \u001B[0mconnection\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1037\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1038\u001B[0;31m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1039\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_connection_guard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    473\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 475\u001B[0;31m                 \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msmart_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    476\u001B[0m                 \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Answer received: {0}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m                 \u001B[0;31m# Happens when a the other end is dead. There might be an empty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.7/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    587\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 589\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    590\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "stages = [\n",
    "    OneHotEncoder(inputCols=['y'], outputCols=['y_enc']),\n",
    "]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_transform = pipeline_model.transform(df)\n",
    "\n",
    "checkpoint_data = False\n",
    "if checkpoint_data:\n",
    "    df_transform.write.parquet('preprocessed_data.parquet')\n",
    "df_transform.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Checkpoint\n",
    "We make a checkpoint of our data to make sure it's all good as well as to make sure we don't have to rerun the preprocessing as it consistently ran out of memory or took too long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b37f66af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b37f66af",
    "outputId": "457e776f-e491-4be1-f757-321cc75950f3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1d3heMiNKlonRTfY4FbDuLSRe-Bdu-4Hq\n",
      "To: /content/preprocessed_data.tar\n",
      "100%|██████████| 138M/138M [00:02<00:00, 59.0MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- X: vector (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      " |-- loc: vector (nullable = true)\n",
      " |-- y_enc: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the partially preprocessed data\n",
    "download_data = True\n",
    "checkpoint_data = False\n",
    "\n",
    "if download_data:\n",
    "    data_tar = 'preprocessed_data.tar'\n",
    "\n",
    "    #Push this one\n",
    "    data_url = 'https://drive.google.com/uc?id=1d3heMiNKlonRTfY4FbDuLSRe-Bdu-4Hq'\n",
    "\n",
    "    if not os.path.exists(data_tar) and not os.path.exists('preprocessed_data.parquet'):\n",
    "        gdown.download(data_url, data_tar, quiet=False)\n",
    "    if not os.path.exists('preprocessed_data.parquet'):\n",
    "        !tar -xf 'preprocessed_data.tar'\n",
    "    checkpoint_data = True\n",
    "\n",
    "# load in the partially preprocessed data\n",
    "if checkpoint_data:\n",
    "    df_transform = spark.read.parquet(\"preprocessed_data.parquet\")\n",
    "df_transform.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5afc6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7d5afc6b"
   },
   "source": [
    "# Final Data Preparation\n",
    "Splits the data into and training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd9ac91b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cd9ac91b"
   },
   "outputs": [],
   "source": [
    "\n",
    "seed = 0  # can remove this in the future, here for now for consistency\n",
    "val_percent = 0.2\n",
    "df_transform = df_transform.orderBy(rand())\n",
    "df_transform_fin = df_transform.select('X', 'y_enc')\n",
    "train, test = df_transform_fin.randomSplit([1-val_percent,val_percent], seed=seed)\n",
    "gridCount = len(train.select('y_enc').first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the Model\n",
    "This defines the model that we will use to train our data.\n",
    "We use resnet50 as our backbone, then feed the output of each of the 3 images into an LSTM module configured for a many to one architecture. We then feed this into more dense layers and finally output the softmaxed prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5ce6011",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a5ce6011"
   },
   "outputs": [],
   "source": [
    "def get_model(useRestnet = True, inputShape=(3, 100, 200, 3), gridCount=39):\n",
    "    '''\n",
    "    The function is used to load or initialize a new model\n",
    "    useRestnet : set to True to use pretrained frozen restnet model\n",
    "                 set to False to use trainable CNN model\n",
    "    inputShape: Shape of input image set\n",
    "                (<numer-of-images>, <image-width>, <image-height>, <RGB-values>)\n",
    "    gridCount: Number of ouput grids to predict on\n",
    "    '''\n",
    "    size = 1\n",
    "\n",
    "    for i in inputShape:\n",
    "        size *= i\n",
    "\n",
    "    convnet = tf.keras.Sequential()\n",
    "    if useRestnet:\n",
    "        # use restnet CNN\n",
    "        restnet = ResNet50(include_top=False, weights='imagenet', input_shape=inputShape[1:])\n",
    "        # Freeze model\n",
    "        restnet.trainable = False\n",
    "        convnet.add(restnet)\n",
    "    else:\n",
    "        # Use trainable CNN\n",
    "        convnet.add(Conv2D(128, (3,3), input_shape=inputShape[1:],\n",
    "                           padding='same', activation='relu'))\n",
    "        convnet.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(BatchNormalization(momentum=.6))\n",
    "        convnet.add(MaxPool2D())\n",
    "        convnet.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(BatchNormalization(momentum=.6))\n",
    "        convnet.add(MaxPool2D())\n",
    "        convnet.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(BatchNormalization(momentum=.6))\n",
    "        convnet.add(MaxPool2D())\n",
    "        convnet.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "        convnet.add(BatchNormalization(momentum=.6))\n",
    "    convnet.add(GlobalMaxPool2D())\n",
    "    model = tf.keras.Sequential()\n",
    "    # Connect the CNN to an LSTM\n",
    "    model.add(Reshape(inputShape,input_shape=(1, size)))\n",
    "    model.add(TimeDistributed(convnet))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(gridCount, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the training parameters and get the model object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dab462",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a5dab462",
    "outputId": "333bd529-d759-44de-c51d-660e6bdfd585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_14 (Reshape)        (None, 3, 100, 200, 3)    0         \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 3, 2048)          23587712  \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                540928    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1024)              66560     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 39)                2535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,796,455\n",
      "Trainable params: 1,208,743\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Training Params\n",
    "lr = 0.001\n",
    "momentum = 0.1  # TODO - potentially use momentum here\n",
    "num_workers = 1\n",
    "epochs = 20\n",
    "batch_size = 5\n",
    "verbosity = 1\n",
    "mode = 'synchronous'\n",
    "\n",
    "# Get model\n",
    "model = get_model(useRestnet=True, inputShape=(3, 100, 200, 3), gridCount=gridCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Spark Keras Wrapper\n",
    "This creates a Spark ml Estimator wrapper around the Keras model that we will then use to train and evaluate the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bef1c276",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bef1c276"
   },
   "outputs": [],
   "source": [
    "class DistKeras(ml.Estimator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.__trainer_klass = args[0]\n",
    "        self.__trainer_params = args[1]\n",
    "        self.__build_trainer(**kwargs)\n",
    "        super().__init__()\n",
    "\n",
    "    @classmethod\n",
    "    def __build_keras_model(klass, *args, **kwargs):\n",
    "        keras_model = get_model(useRestnet=True, inputShape=(3, 100, 200, 3), gridCount=gridCount)\n",
    "        return keras_model\n",
    "\n",
    "    def __build_trainer(self, *args, **kwargs):\n",
    "        print(kwargs)\n",
    "        loss = kwargs['loss']\n",
    "        keras_model = DistKeras.__build_keras_model(**kwargs)\n",
    "        self._trainer = self.__trainer_klass(keras_model, 'adam',\n",
    "                                             loss, **self.__trainer_params)\n",
    "\n",
    "    def _fit(self, *args, **kwargs):\n",
    "        data_frame = args[0]\n",
    "        if len(args) > 1:\n",
    "            params = args[1]\n",
    "            self.__build_trainer(**params)\n",
    "        keras_model = self._trainer.train(data_frame)\n",
    "        return DistKerasModel(keras_model)\n",
    "\n",
    "\n",
    "class DistKerasModel(ml.Model):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._keras_model = args[0]\n",
    "        self._predictor = predictors.ModelPredictor(self._keras_model)\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, *args, **kwargs):\n",
    "        data_frame = args[0]\n",
    "        pred_col = self._predictor.output_column\n",
    "        preds = self._predictor.predict(data_frame)\n",
    "        return preds.withColumn(pred_col,\n",
    "                                cast_to_double(preds[pred_col]))\n",
    "\n",
    "\n",
    "cast_to_double = functions.udf(lambda row: float(row[0]), types.DoubleType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Spark Keras Estimator\n",
    "We create the distributed estimator then fit it and transform it to the training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c774510b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c774510b",
    "outputId": "8bf8e5af-b7f3-4d6a-c0d0-bd22be65d1f7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 'categorical_crossentropy', 'lr_decay': 0.9, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/distkeras/parameter_servers.py:274: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.center_variable = np.asarray(self.model.get_weights())\n"
     ]
    }
   ],
   "source": [
    "port = 6980\n",
    "port += 1\n",
    "estimator = DistKeras(trainers.ADAG,\n",
    "                      {'batch_size': 256,\n",
    "                       'communication_window': 3,\n",
    "                       'num_epoch': 10,\n",
    "                       'num_workers': 1,\n",
    "                       'master_port': port,\n",
    "                       'features_col': 'X',\n",
    "                       'label_col': 'y_enc'}, loss='categorical_crossentropy',lr_decay=0.90, learning_rate=1e-2)\n",
    "estimator = estimator.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell was unsuccessful as we could not get the distributed estimator to work."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97aa078a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "97aa078a",
    "outputId": "8ac26f30-4775-438c-95b7-c5bb2569cd1e"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-64-36c054f69164>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    215\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Params must be a param map but got %s.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-43-9cb91282f3f3>\u001B[0m in \u001B[0;36m_transform\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0mdata_frame\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0mpred_col\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predictor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_column\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predictor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_frame\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m         return preds.withColumn(pred_col,\n\u001B[1;32m     60\u001B[0m                                 cast_to_double(preds[pred_col]))\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/distkeras/predictors.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, dataframe)\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0mprediction\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \"\"\"\n\u001B[0;32m---> 68\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mdataframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mtoDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m         \"\"\"\n\u001B[0;32m---> 66\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msampleRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0mRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtoDF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    673\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    674\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    676\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    696\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 698\u001B[0;31m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    699\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    484\u001B[0m         \"\"\"\n\u001B[1;32m    485\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 486\u001B[0;31m             \u001B[0mstruct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inferSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    487\u001B[0m             \u001B[0mconverter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_converter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstruct\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    488\u001B[0m             \u001B[0mrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    458\u001B[0m         \u001B[0;34m:\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStructType\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m         \"\"\"\n\u001B[0;32m--> 460\u001B[0;31m         \u001B[0mfirst\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirst\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    461\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfirst\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    462\u001B[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mfirst\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1586\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRDD\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mempty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1587\u001B[0m         \"\"\"\n\u001B[0;32m-> 1588\u001B[0;31m         \u001B[0mrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1589\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1590\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1567\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1568\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1569\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1570\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1225\u001B[0m         \u001B[0;31m# SparkContext#runJob.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1226\u001B[0m         \u001B[0mmappedRDD\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionFunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1227\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1228\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1229\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1322\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1324\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 56) (13062fbdf5d8 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1560, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/distkeras/predictors.py\", line 58, in _predict\n    features = [np.asarray([row[c]]) for c in self.features_column]\n  File \"/usr/local/lib/python3.7/dist-packages/distkeras/predictors.py\", line 58, in <listcomp>\n    features = [np.asarray([row[c]]) for c in self.features_column]\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1565, in __getitem__\n    raise ValueError(item)\nValueError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1560, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/distkeras/predictors.py\", line 58, in _predict\n    features = [np.asarray([row[c]]) for c in self.features_column]\n  File \"/usr/local/lib/python3.7/dist-packages/distkeras/predictors.py\", line 58, in <listcomp>\n    features = [np.asarray([row[c]]) for c in self.features_column]\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1565, in __getitem__\n    raise ValueError(item)\nValueError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "estimator.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc57505",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5cc57505"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1f93a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5be1f93a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430765d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3430765d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcj5nKJVH48e",
    "outputId": "e43fba3b-e6f1-44f7-cd6e-0a6b80a28d07",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "dcj5nKJVH48e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ozsXiIzvJEqO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "ozsXiIzvJEqO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "name": "temporary_main.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}